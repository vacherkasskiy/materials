{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Градиентный спуск — это широко используемый метод оптимизации, особенно в задачах машинного обучения и глубокого обучения. Он заключается в нахождении минимума (или максимума) функции путём итеративного обновления её параметров в направлении антиградиента (или градиента) функции потерь. Давайте рассмотрим основные аспекты градиентного спуска и его модификации.\n",
        "\n",
        "### Основная идея градиентного спуска\n",
        "\n",
        "Для функции \\( f(\\theta) \\) с параметрами \\( \\theta \\) градиентный спуск обновляет параметры по следующему правилу:\n",
        "\n",
        "\\[ \\theta = \\theta - \\eta \\nabla f(\\theta) \\]\n",
        "\n",
        "где:\n",
        "- \\( \\theta \\) — параметры модели,\n",
        "- \\( \\eta \\) — шаг обучения (learning rate),\n",
        "- \\( \\nabla f(\\theta) \\) — градиент функции потерь по параметрам.\n",
        "\n",
        "### Варианты градиентного спуска\n",
        "\n",
        "1. **Пакетный (Batch) градиентный спуск**:\n",
        "   - Обновляет параметры после обработки всего обучающего набора данных.\n",
        "   - Преимущества: Стабильные обновления, гарантированное снижение функции потерь.\n",
        "   - Недостатки: Медленность при больших наборах данных.\n",
        "\n",
        "2. **Стохастический (Stochastic) градиентный спуск (SGD)**:\n",
        "   - Обновляет параметры после обработки каждого отдельного примера из обучающего набора данных.\n",
        "   - Преимущества: Быстрая обработка, позволяет избегать локальных минимумов.\n",
        "   - Недостатки: Шумные обновления, может колебаться вокруг минимума.\n",
        "\n",
        "3. **Мини-пакетный (Mini-batch) градиентный спуск**:\n",
        "   - Компромисс между пакетным и стохастическим градиентным спуском. Обновляет параметры после обработки небольшого подмножества (батча) данных.\n",
        "   - Преимущества: Быстрее пакетного, менее шумный, чем SGD.\n",
        "   - Недостатки: Необходимость выбора размера батча.\n",
        "\n",
        "### Модификации градиентного спуска\n",
        "\n",
        "1. **Momentum (моментум)**:\n",
        "   - Учитывает предыдущие градиенты для ускорения обучения.\n",
        "   - Обновление параметров:\n",
        "     \\[ v = \\beta v + (1 - \\beta) \\nabla f(\\theta) \\]\n",
        "     \\[ \\theta = \\theta - \\eta v \\]\n",
        "   - \\( \\beta \\) — параметр сглаживания, обычно близкий к 1.\n",
        "\n",
        "2. **Nesterov Accelerated Gradient (NAG)**:\n",
        "   - Улучшенная версия моментума, учитывает \"предвосхищение\" градиента.\n",
        "   - Обновление параметров:\n",
        "     \\[ v = \\beta v + (1 - \\beta) \\nabla f(\\theta - \\eta \\beta v) \\]\n",
        "     \\[ \\theta = \\theta - \\eta v \\]\n",
        "\n",
        "3. **AdaGrad**:\n",
        "   - Адаптирует шаг обучения для каждого параметра на основе частоты обновлений.\n",
        "   - Обновление параметров:\n",
        "     \\[ g_t = \\nabla f(\\theta_t) \\]\n",
        "     \\[ G_t = G_{t-1} + g_t^2 \\]\n",
        "     \\[ \\theta = \\theta - \\frac{\\eta}{\\sqrt{G_t} + \\epsilon} g_t \\]\n",
        "   - \\( G_t \\) — сумма квадратов градиентов, \\( \\epsilon \\) — малое значение для предотвращения деления на ноль.\n",
        "\n",
        "4. **RMSProp**:\n",
        "   - Модификация AdaGrad с экспоненциальным затуханием среднего квадрата градиентов.\n",
        "   - Обновление параметров:\n",
        "     \\[ E[g^2]_t = \\beta E[g^2]_{t-1} + (1 - \\beta) g_t^2 \\]\n",
        "     \\[ \\theta = \\theta - \\frac{\\eta}{\\sqrt{E[g^2]_t} + \\epsilon} g_t \\]\n",
        "\n",
        "5. **Adam (Adaptive Moment Estimation)**:\n",
        "   - Комбинирует идеи RMSProp и моментума.\n",
        "   - Обновление параметров:\n",
        "     \\[ m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t \\]\n",
        "     \\[ v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2 \\]\n",
        "     \\[ \\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t} \\]\n",
        "     \\[ \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t} \\]\n",
        "     \\[ \\theta = \\theta - \\eta \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon} \\]\n",
        "\n",
        "### Выбор метода\n",
        "\n",
        "- **SGD** и его варианты (мини-пакетный градиентный спуск) часто используются для больших наборов данных.\n",
        "- **Momentum** и **NAG** ускоряют сходжение, особенно в задачах глубокого обучения.\n",
        "- **AdaGrad** подходит для задач с разреженными данными.\n",
        "- **RMSProp** и **Adam** являются наиболее популярными методами, так как они хорошо работают в большинстве случаев и не требуют сложной настройки гиперпараметров.\n",
        "\n",
        "Эти методы помогают эффективно и точно находить оптимальные параметры моделей, улучшая их производительность и ускоряя обучение."
      ],
      "metadata": {
        "id": "8sbV6UiUYNAJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Шаг 1: Импортируем необходимые библиотеки\n",
        "# (библиотеки уже импортированы выше)\n",
        "\n",
        "# Шаг 2: Загрузим датасет Iris и подготовим данные\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Используем только два класса для бинарной классификации\n",
        "X = X[y != 2]\n",
        "y = y[y != 2]\n",
        "\n",
        "# Разделим данные на обучающую и тестовую выборки\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Стандартизируем данные\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Шаг 3: Определим стохастический градиентный спуск с адаптивным градиентом (AdaGrad)\n",
        "# Используем SGDClassifier из scikit-learn с параметром learning_rate='optimal' для реализации AdaGrad\n",
        "sgd_adagrad = SGDClassifier(loss='log', penalty='l2', max_iter=1000, tol=1e-3, learning_rate='optimal', eta0=0.01)\n",
        "\n",
        "# Шаг 4: Обучим модель и оценим её производительность\n",
        "sgd_adagrad.fit(X_train, y_train)\n",
        "y_pred = sgd_adagrad.predict(X_test)\n",
        "\n",
        "# Оценим точность модели\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Дополнительно: Выводим коэффициенты модели\n",
        "print(\"Model coefficients:\", sgd_adagrad.coef_)\n",
        "print(\"Model intercept:\", sgd_adagrad.intercept_)\n"
      ],
      "metadata": {
        "id": "fD9j1YR3YN9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Решающее дерево (Decision Tree)\n",
        "\n",
        "Решающее дерево — это модель машинного обучения, которая используется для классификации и регрессии. Оно представляет собой дерево, где каждый узел обозначает проверку атрибута, каждая ветвь — результат проверки, а каждый лист — метку класса или значение регрессии.\n",
        "\n",
        "#### Пример кода в Google Colab\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Загрузка данных\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Разделение на тренировочную и тестовую выборки\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Создание и обучение модели\n",
        "tree = DecisionTreeClassifier()\n",
        "tree.fit(X_train, y_train)\n",
        "\n",
        "# Предсказание и оценка модели\n",
        "y_pred = tree.predict(X_test)\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
        "```\n",
        "\n",
        "### Бустинг (Boosting)\n",
        "\n",
        "Бустинг — это метод ансамблевого обучения, который создает сильную модель, объединяя несколько слабых моделей (обычно деревьев решений). В процессе бустинга модели обучаются последовательно, и каждая новая модель исправляет ошибки предыдущих.\n",
        "\n",
        "#### Модификации бустинга\n",
        "\n",
        "1. **XGBoost**:\n",
        "   - Оптимизированный градиентный бустинг, который использует регуляризацию и параллельные вычисления.\n",
        "\n",
        "2. **LightGBM**:\n",
        "   - Градиентный бустинг, который использует методы для уменьшения времени обучения и потребления памяти. Он строит деревья, используя листовой рост.\n",
        "\n",
        "3. **CatBoost**:\n",
        "   - Градиентный бустинг, который специально оптимизирован для категориальных признаков и работает без их предварительного кодирования.\n",
        "\n",
        "#### Пример кода XGBoost в Google Colab\n",
        "\n",
        "```python\n",
        "!pip install xgboost\n",
        "\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Загрузка данных\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Разделение на тренировочную и тестовую выборки\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Создание и обучение модели\n",
        "model = xgb.XGBClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Предсказание и оценка модели\n",
        "y_pred = model.predict(X_test)\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
        "```\n",
        "\n",
        "### Случайный лес (Random Forest)\n",
        "\n",
        "Случайный лес — это метод ансамблевого обучения, который создает множество решающих деревьев и объединяет их предсказания. Он использует случайное подмножество признаков для обучения каждого дерева, что уменьшает переобучение.\n",
        "\n",
        "#### Пример кода в Google Colab\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Загрузка данных\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Разделение на тренировочную и тестовую выборки\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Создание и обучение модели\n",
        "forest = RandomForestClassifier(n_estimators=100)\n",
        "forest.fit(X_train, y_train)\n",
        "\n",
        "# Предсказание и оценка модели\n",
        "y_pred = forest.predict(X_test)\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
        "```\n",
        "\n",
        "### Бэггинг (Bagging)\n",
        "\n",
        "Бэггинг (Bootstrap Aggregating) — это метод ансамблевого обучения, который создает множество моделей, обученных на разных подвыборках обучающих данных, и усредняет их предсказания. Самая известная реализация — это случайный лес.\n",
        "\n",
        "#### Пример кода в Google Colab\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "\n",
        "# Загрузка данных\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Разделение на тренировочную и тестовую выборки\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Создание и обучение модели\n",
        "bagging = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100)\n",
        "bagging.fit(X_train, y_train)\n",
        "\n",
        "# Предсказание и оценка модели\n",
        "y_pred = bagging.predict(X_test)\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
        "```\n",
        "\n",
        "### Сравнение методов\n",
        "\n",
        "- **Решающее дерево**: Быстрое и интерпретируемое, но может переобучаться.\n",
        "- **Бустинг**: Высокая точность, но долгое время обучения.\n",
        "- **Случайный лес**: Стабильность и высокая точность, быстрая работа.\n",
        "- **Бэггинг**: Улучшает стабильность и уменьшает переобучение.\n",
        "\n",
        "Каждый метод имеет свои особенности и лучше всего подходит для определенных типов задач и данных."
      ],
      "metadata": {
        "id": "ThS7GmaVal9F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для использования CatBoost в Google Colab потребуется установить библиотеку `catboost`, загрузить данные и создать модель. Давайте рассмотрим пример на датасете `Iris`.\n",
        "\n",
        "### Пример использования CatBoost в Google Colab\n",
        "\n",
        "1. Установка библиотеки `catboost`.\n",
        "\n",
        "```python\n",
        "!pip install catboost\n",
        "```\n",
        "\n",
        "2. Импорт необходимых библиотек и загрузка данных.\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Загрузка данных\n",
        "from sklearn.datasets import load_iris\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Преобразование данных в DataFrame для более удобного использования\n",
        "df = pd.DataFrame(X, columns=data.feature_names)\n",
        "df['target'] = y\n",
        "\n",
        "# Разделение на тренировочную и тестовую выборки\n",
        "X_train, X_test, y_train, y_test = train_test_split(df.drop('target', axis=1), df['target'], test_size=0.2, random_state=42)\n",
        "```\n",
        "\n",
        "3. Обучение и оценка модели CatBoost.\n",
        "\n",
        "```python\n",
        "# Создание и обучение модели CatBoost\n",
        "model = CatBoostClassifier(iterations=100, learning_rate=0.1, depth=6, verbose=0)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Предсказание и оценка модели\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "```\n",
        "\n",
        "4. Использование `Pool` для работы с категориальными признаками (если они есть).\n",
        "\n",
        "Если бы у нас были категориальные признаки, мы могли бы использовать `Pool`, чтобы указать CatBoost, какие столбцы являются категориальными.\n",
        "\n",
        "```python\n",
        "# Пример данных с категориальными признаками\n",
        "data = {\n",
        "    'numerical_feature': [1, 2, 3, 4, 5, 6],\n",
        "    'categorical_feature': ['A', 'B', 'A', 'A', 'B', 'B'],\n",
        "    'target': [0, 1, 0, 1, 0, 1]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "X = df[['numerical_feature', 'categorical_feature']]\n",
        "y = df['target']\n",
        "\n",
        "# Разделение данных\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Создание Pool с указанием категориальных признаков\n",
        "train_pool = Pool(X_train, y_train, cat_features=['categorical_feature'])\n",
        "test_pool = Pool(X_test, y_test, cat_features=['categorical_feature'])\n",
        "\n",
        "# Создание и обучение модели CatBoost\n",
        "model = CatBoostClassifier(iterations=100, learning_rate=0.1, depth=6, verbose=0)\n",
        "model.fit(train_pool)\n",
        "\n",
        "# Предсказание и оценка модели\n",
        "y_pred = model.predict(test_pool)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "```\n",
        "\n",
        "В этом примере:\n",
        "- Мы установили библиотеку `catboost`.\n",
        "- Загрузили данные и разделили их на тренировочные и тестовые выборки.\n",
        "- Обучили модель `CatBoostClassifier`.\n",
        "- Оценили точность модели.\n",
        "\n",
        "CatBoost автоматически обрабатывает категориальные признаки и часто показывает хорошие результаты на данных с такими признаками. В примере с `Pool` показано, как указать категориальные признаки для улучшения обработки данных."
      ],
      "metadata": {
        "id": "6QYL8qRgaqOP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$a$\n",
        "\n",
        "## Метрики для обучения модели\n",
        "\n",
        "### Метрики для регрессии\n",
        "\n",
        "1. **Mean Squared Error (MSE)**\n",
        "   - Описывает среднее значение квадратов ошибок между предсказанными и реальными значениями.\n",
        "   - Формула:\n",
        "     $$\n",
        "     text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
        "     $$\n",
        "   - Используется, когда важно минимизировать большие ошибки, так как квадраты увеличивают вес больших отклонений.\n",
        "\n",
        "### Метрики для классификации\n",
        "\n",
        "1. **Accuracy**\n",
        "   - Процент правильно классифицированных примеров.\n",
        "   - Формула:\n",
        "     $$\n",
        "     \\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
        "     $$\n",
        "   - Где:\n",
        "     - \\(TP\\) (True Positive): Истинные положительные\n",
        "     - \\(TN\\) (True Negative): Истинные отрицательные\n",
        "     - \\(FP\\) (False Positive): Ложные положительные\n",
        "     - \\(FN\\) (False Negative): Ложные отрицательные\n",
        "   - Используется, когда классы сбалансированы и одинаково важны.\n",
        "\n",
        "2. **Precision**\n",
        "   - Доля правильно предсказанных положительных примеров среди всех предсказанных положительных примеров.\n",
        "   - Формула:\n",
        "     $$\n",
        "     \\text{Precision} = \\frac{TP}{TP + FP}\n",
        "     $$\n",
        "   - Используется, когда важна точность предсказания положительного класса (например, при детекции спама).\n",
        "\n",
        "3. **Recall** (Sensitivity, True Positive Rate)\n",
        "   - Доля правильно предсказанных положительных примеров среди всех реальных положительных примеров.\n",
        "   - Формула:\n",
        "     $$\n",
        "     \\text{Recall} = \\frac{TP}{TP + FN}\n",
        "     $$\n",
        "   - Используется, когда важно не пропустить положительный класс (например, при медицинской диагностике).\n",
        "\n",
        "4. **F1-Score**\n",
        "   - Гармоническое среднее Precision и Recall.\n",
        "   - Формула:\n",
        "     $$\n",
        "     \\text{F1-Score} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
        "     $$\n",
        "   - Используется, когда нужно сбалансировать Precision и Recall (например, при выявлении мошенничества).\n",
        "\n",
        "5. **ROC-AUC (Receiver Operating Characteristic - Area Under Curve)**\n",
        "   - Площадь под кривой ROC, которая отображает зависимость между TPR (True Positive Rate) и FPR (False Positive Rate).\n",
        "   - Используется, когда нужно оценить модель по всем возможным порогам классификации. Хорош для несбалансированных классов.\n",
        "\n",
        "### Другие метрики\n",
        "\n",
        "1. **Logarithmic Loss (Log Loss)**\n",
        "   - Используется для оценки моделей классификации на основе вероятностных предсказаний.\n",
        "   - Формула:\n",
        "     $$\n",
        "     \\text{Log Loss} = - \\frac{1}{n} \\sum_{i=1}^{n} [y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i)]\n",
        "     $$\n",
        "\n",
        "2. **Mean Absolute Error (MAE)**\n",
        "   - Среднее абсолютное отклонение между предсказанными и реальными значениями.\n",
        "   - Формула:\n",
        "     $$\n",
        "     \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n",
        "     $$\n",
        "\n",
        "3. **R^2 Score (Coefficient of Determination)**\n",
        "   - Определяет, какая часть дисперсии зависимой переменной объясняется моделью.\n",
        "   - Формула:\n",
        "     $$\n",
        "     R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}\n",
        "     $$\n",
        "\n",
        "### Примеры использования метрик\n",
        "\n",
        "1. **MSE**\n",
        "   - Используется в задачах регрессии, например, для предсказания цен недвижимости, где большие ошибки имеют значение.\n",
        "\n",
        "2. **ROC-AUC**\n",
        "   - Используется в задачах классификации с несбалансированными классами, например, при выявлении редких заболеваний.\n",
        "\n",
        "3. **F1-Score**\n",
        "   - Используется, когда важно сбалансировать Precision и Recall, например, при выявлении мошенничества.\n",
        "\n",
        "4. **Accuracy**\n",
        "   - Используется в задачах с равными и сбалансированными классами, например, для классификации изображений животных.\n",
        "\n",
        "### Пример кода в Google Colab"
      ],
      "metadata": {
        "id": "fLO0JiEmctMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Загрузка данных\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Преобразование задачи в бинарную классификацию\n",
        "y = (y == 0).astype(int)\n",
        "\n",
        "# Разделение на тренировочную и тестовую выборки\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Обучение модели\n",
        "model = RandomForestClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Предсказание\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Вычисление метрик\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-Score: {f1:.2f}\")\n",
        "print(f\"ROC-AUC: {roc_auc:.2f}\")"
      ],
      "metadata": {
        "id": "1U8OLUWkhnN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Линейная регрессия\n",
        "\n",
        "Линейная регрессия – это один из наиболее простых и широко используемых методов регрессии, который моделирует линейные отношения между зависимой переменной (целью) и одной или несколькими независимыми переменными (признаками).\n",
        "\n",
        "#### Основная идея\n",
        "\n",
        "Модель линейной регрессии пытается найти линейное уравнение:\n",
        "\n",
        "\\[ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_n x_n + \\epsilon \\]\n",
        "\n",
        "где:\n",
        "- \\( y \\) – зависимая переменная (целевая переменная).\n",
        "- \\( x_1, x_2, \\ldots, x_n \\) – независимые переменные (признаки).\n",
        "- \\( \\beta_0 \\) – свободный член (интерсепт).\n",
        "- \\( \\beta_1, \\beta_2, \\ldots, \\beta_n \\) – коэффициенты регрессии.\n",
        "- \\( \\epsilon \\) – ошибка модели (разница между предсказанными и реальными значениями).\n",
        "\n",
        "#### Пример использования линейной регрессии\n",
        "\n",
        "Пример на Python с использованием библиотеки `scikit-learn`:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Создание искусственного набора данных\n",
        "np.random.seed(42)\n",
        "X = 2 * np.random.rand(100, 1)\n",
        "y = 4 + 3 * X + np.random.randn(100, 1)\n",
        "\n",
        "# Разделение данных на тренировочную и тестовую выборки\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Обучение модели линейной регрессии\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Предсказание и оценка модели\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "```\n",
        "\n",
        "### Другие виды регрессии\n",
        "\n",
        "Помимо линейной регрессии, существуют и другие методы регрессии, которые могут быть более эффективными в зависимости от задачи.\n",
        "\n",
        "#### 1. Полиномиальная регрессия\n",
        "\n",
        "Моделирует нелинейные отношения путем добавления полиномиальных признаков. Полиномиальная регрессия представляет собой линейную регрессию с полиномиальными признаками.\n",
        "\n",
        "\\[ y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\ldots + \\beta_n x^n + \\epsilon \\]\n",
        "\n",
        "Пример на Python:\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Преобразование признаков в полиномиальные\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X_train)\n",
        "\n",
        "# Обучение модели на полиномиальных признаках\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, y_train)\n",
        "\n",
        "# Предсказание на тестовой выборке\n",
        "X_test_poly = poly.transform(X_test)\n",
        "y_pred = model.predict(X_test_poly)\n",
        "```\n",
        "\n",
        "#### 2. Лассо (Lasso) регрессия\n",
        "\n",
        "Линейная регрессия с L1-регуляризацией, которая добавляет штраф за абсолютное значение коэффициентов, что помогает избежать переобучения и может занулять некоторые коэффициенты.\n",
        "\n",
        "\\[ \\text{Lasso: } \\min \\left( \\frac{1}{2n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\alpha \\sum_{j=1}^{p} |\\beta_j| \\right) \\]\n",
        "\n",
        "Пример на Python:\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "# Обучение модели Lasso регрессии\n",
        "model = Lasso(alpha=0.1)\n",
        "model.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "#### 3. Гребневая регрессия (Ridge Regression)\n",
        "\n",
        "Линейная регрессия с L2-регуляризацией, которая добавляет штраф за квадраты коэффициентов, что также помогает избежать переобучения.\n",
        "\n",
        "\\[ \\text{Ridge: } \\min \\left( \\frac{1}{2n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\alpha \\sum_{j=1}^{p} \\beta_j^2 \\right) \\]\n",
        "\n",
        "Пример на Python:\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "# Обучение модели Ridge регрессии\n",
        "model = Ridge(alpha=1.0)\n",
        "model.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "#### 4. Регрессия Elastic Net\n",
        "\n",
        "Комбинирует L1 и L2-регуляризацию, что позволяет получить преимущества обеих техник.\n",
        "\n",
        "\\[ \\text{Elastic Net: } \\min \\left( \\frac{1}{2n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\alpha \\left( \\rho \\sum_{j=1}^{p} |\\beta_j| + \\frac{1 - \\rho}{2} \\sum_{j=1}^{p} \\beta_j^2 \\right) \\right) \\]\n",
        "\n",
        "Пример на Python:\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import ElasticNet\n",
        "\n",
        "# Обучение модели Elastic Net регрессии\n",
        "model = ElasticNet(alpha=1.0, l1_ratio=0.5)\n",
        "model.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "#### 5. Регрессия с опорными векторами (Support Vector Regression, SVR)\n",
        "\n",
        "Использует метод опорных векторов для регрессии, эффективно моделирует нелинейные зависимости.\n",
        "\n",
        "Пример на Python:\n",
        "\n",
        "```python\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "# Обучение модели SVR\n",
        "model = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=0.1)\n",
        "model.fit(X_train, y_train.ravel())\n",
        "```\n",
        "\n",
        "### Заключение\n",
        "\n",
        "Методы регрессии выбираются в зависимости от задачи и данных. Линейная регрессия является простым и понятным методом, но для более сложных зависимостей могут потребоваться методы с регуляризацией или нелинейные методы, такие как полиномиальная регрессия и регрессия с опорными векторами."
      ],
      "metadata": {
        "id": "fNSG7Wv6hqwU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Когда использовать различные виды регрессии\n",
        "\n",
        "Различные методы регрессии применяются в зависимости от природы данных, их сложности и специфических требований задачи. Вот более детальный обзор с примерами:\n",
        "\n",
        "#### 1. Линейная регрессия\n",
        "\n",
        "**Когда использовать**:\n",
        "- Данные имеют линейную зависимость.\n",
        "- Простая модель с малым количеством параметров.\n",
        "- Интерпретируемость модели важна.\n",
        "\n",
        "**Пример**:\n",
        "- Предсказание дохода на основе количества отработанных часов.\n",
        "\n",
        "```python\n",
        "# Пример: Предсказание цены дома на основе его площади\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Создание искусственного набора данных\n",
        "np.random.seed(42)\n",
        "X = 2 * np.random.rand(100, 1)\n",
        "y = 4 + 3 * X + np.random.randn(100, 1)\n",
        "\n",
        "# Разделение данных на тренировочную и тестовую выборки\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Обучение модели линейной регрессии\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Предсказание и оценка модели\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "```\n",
        "\n",
        "#### 2. Полиномиальная регрессия\n",
        "\n",
        "**Когда использовать**:\n",
        "- Данные имеют нелинейную зависимость.\n",
        "- Модель должна учитывать взаимодействие между признаками.\n",
        "\n",
        "**Пример**:\n",
        "- Предсказание роста растения в зависимости от времени и условий среды.\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Преобразование признаков в полиномиальные\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X_train)\n",
        "\n",
        "# Обучение модели на полиномиальных признаках\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, y_train)\n",
        "\n",
        "# Предсказание на тестовой выборке\n",
        "X_test_poly = poly.transform(X_test)\n",
        "y_pred = model.predict(X_test_poly)\n",
        "```\n",
        "\n",
        "#### 3. Лассо (Lasso) регрессия\n",
        "\n",
        "**Когда использовать**:\n",
        "- Высокая многомерность данных (много признаков).\n",
        "- Необходимость отбора признаков (несколько признаков могут быть занулены).\n",
        "\n",
        "**Пример**:\n",
        "- Предсказание стоимости квартиры на основе множества факторов (площадь, этаж, район и т.д.).\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "# Обучение модели Lasso регрессии\n",
        "model = Lasso(alpha=0.1)\n",
        "model.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "#### 4. Гребневая регрессия (Ridge Regression)\n",
        "\n",
        "**Когда использовать**:\n",
        "- Высокая многомерность данных.\n",
        "- Признаки коррелированы между собой.\n",
        "- Необходимость избежать переобучения.\n",
        "\n",
        "**Пример**:\n",
        "- Предсказание продаж на основе исторических данных и маркетинговых расходов.\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "# Обучение модели Ridge регрессии\n",
        "model = Ridge(alpha=1.0)\n",
        "model.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "#### 5. Регрессия Elastic Net\n",
        "\n",
        "**Когда использовать**:\n",
        "- Высокая многомерность данных.\n",
        "- Сочетание преимуществ L1 и L2-регуляризации.\n",
        "- Признаки коррелированы и нужно отбирать значимые признаки.\n",
        "\n",
        "**Пример**:\n",
        "- Предсказание качества вина на основе химических свойств и экспертовых оценок.\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import ElasticNet\n",
        "\n",
        "# Обучение модели Elastic Net регрессии\n",
        "model = ElasticNet(alpha=1.0, l1_ratio=0.5)\n",
        "model.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "#### 6. Регрессия с опорными векторами (Support Vector Regression, SVR)\n",
        "\n",
        "**Когда использовать**:\n",
        "- Нелинейные зависимости в данных.\n",
        "- Малый набор данных.\n",
        "- Необходимость в гибкости модели и её устойчивости к выбросам.\n",
        "\n",
        "**Пример**:\n",
        "- Предсказание спроса на продукт в зависимости от времени года и экономических факторов.\n",
        "\n",
        "```python\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "# Обучение модели SVR\n",
        "model = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=0.1)\n",
        "model.fit(X_train, y_train.ravel())\n",
        "```\n",
        "\n",
        "### Заключение\n",
        "\n",
        "При выборе метода регрессии важно учитывать структуру данных, наличие или отсутствие линейных зависимостей, количество и корреляцию признаков, а также требования к интерпретируемости модели. Линейная регрессия хороша для простых задач с линейными зависимостями, тогда как методы с регуляризацией (Lasso, Ridge, Elastic Net) и нелинейные методы (полиномиальная регрессия, SVR) лучше подходят для сложных задач с большим количеством признаков и нелинейными зависимостями."
      ],
      "metadata": {
        "id": "-Q9P5cn5iwg2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Разложение ошибки на шум, смещение и разброс\n",
        "Когда мы говорим о качестве модели машинного обучения, важно понимать, откуда берется ошибка модели. Общая ошибка модели можно разложить на три составляющие: шум, смещение (bias) и разброс (variance).\n",
        "\n",
        "1. Шум (Noise)\n",
        "Шум – это непредсказуемая часть ошибки, которая возникает из-за сложности или случайности данных. Шум не может быть уменьшен с помощью обучения модели, так как он связан с непредсказуемыми изменениями в данных.\n",
        "\n",
        "2. Смещение (Bias)\n",
        "Смещение – это ошибка, которая возникает, когда модель слишком проста и не может адекватно захватить истинные зависимости в данных. Высокое смещение приводит к недообучению модели. Примером может быть использование линейной модели для описания нелинейной зависимости.\n",
        "\n",
        "3. Разброс (Variance)\n",
        "Разброс – это ошибка, которая возникает, когда модель слишком сложна и сильно зависит от конкретных обучающих данных. Высокий разброс приводит к переобучению модели. Примером может быть использование полинома высокой степени для описания данных с небольшой линейной зависимостью.\n",
        "\n",
        "## Bootstrap\n",
        "Bootstrap – это метод статистической оценки, который использует повторяющуюся выборку с возвращением для оценки точности статистических оценок (например, среднего, медианы и т.д.).\n",
        "\n",
        "## Принцип работы\n",
        "\n",
        "- Из исходного набора данных многократно создаются подвыборки путем случайной выборки с возвращением.\n",
        "- На каждой подвыборке оценивается нужный статистический показатель.\n",
        "- Эти оценки объединяются для получения распределения оценки."
      ],
      "metadata": {
        "id": "L4EWaOUPjswj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Кластеризация\n",
        "\n",
        "Кластеризация – это метод машинного обучения, используемый для группировки объектов на основе их сходства. Она помогает найти скрытые структуры в данных, разделяя их на несколько групп (кластеров), где объекты внутри каждого кластера более похожи друг на друга, чем на объекты из других кластеров.\n",
        "\n",
        "### Основные алгоритмы кластеризации\n",
        "\n",
        "#### 1. K-means\n",
        "\n",
        "K-means – один из самых популярных алгоритмов кластеризации. Он работает, разделяя данные на \\( k \\) кластеров, где \\( k \\) – заранее заданное число.\n",
        "\n",
        "**Шаги алгоритма K-means**:\n",
        "1. Инициализация: Выбрать \\( k \\) начальных центроидов случайным образом.\n",
        "2. Ассигнование: Назначить каждый объект к ближайшему центроиду.\n",
        "3. Обновление: Пересчитать центроиды как среднее всех объектов, присвоенных каждому кластеру.\n",
        "4. Повторение шагов 2 и 3 до тех пор, пока центроиды не перестанут изменяться.\n",
        "\n",
        "**Преимущества**:\n",
        "- Простота и скорость.\n",
        "- Хорошо работает для больших наборов данных.\n",
        "\n",
        "**Недостатки**:\n",
        "- Требует заранее заданного числа кластеров \\( k \\).\n",
        "- Чувствителен к начальному положению центроидов.\n",
        "- Может застрять в локальных минимумах.\n",
        "\n",
        "**Пример использования K-means на Python**:\n",
        "\n",
        "```python\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Создание искусственных данных\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 2)\n",
        "\n",
        "# Применение K-means\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "kmeans.fit(X)\n",
        "labels = kmeans.labels_\n",
        "\n",
        "# Визуализация результатов\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "#### 2. DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\n",
        "\n",
        "DBSCAN – алгоритм кластеризации, основанный на плотности. Он группирует точки, расположенные близко друг к другу, и определяет выбросы (noise points).\n",
        "\n",
        "**Шаги алгоритма DBSCAN**:\n",
        "1. Определить точки ядра (core points), которые имеют не менее \\( minPts \\) соседей в радиусе \\( \\epsilon \\).\n",
        "2. Объединить точки ядра и их соседей в кластеры.\n",
        "3. Обозначить точки, не попавшие в кластеры, как выбросы.\n",
        "\n",
        "**Преимущества**:\n",
        "- Не требует заранее заданного числа кластеров.\n",
        "- Может обнаруживать кластеры произвольной формы.\n",
        "- Устойчив к выбросам.\n",
        "\n",
        "**Недостатки**:\n",
        "- Трудно выбрать подходящие значения \\( \\epsilon \\) и \\( minPts \\).\n",
        "- Не работает хорошо с кластерами переменной плотности.\n",
        "\n",
        "**Пример использования DBSCAN на Python**:\n",
        "\n",
        "```python\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# Применение DBSCAN\n",
        "dbscan = DBSCAN(eps=0.1, min_samples=5)\n",
        "labels = dbscan.fit_predict(X)\n",
        "\n",
        "# Визуализация результатов\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "#### 3. Иерархическая кластеризация\n",
        "\n",
        "Иерархическая кластеризация создаёт иерархию кластеров, объединяя или разделяя их на разных уровнях.\n",
        "\n",
        "**Виды иерархической кластеризации**:\n",
        "- **Агломеративная** (снизу-вверх): Начинается с каждого объекта в отдельном кластере и объединяет пары кластеров.\n",
        "- **Дивизионная** (сверху-вниз): Начинается с одного кластера, содержащего все объекты, и последовательно разделяет кластеры.\n",
        "\n",
        "**Шаги агломеративной кластеризации**:\n",
        "1. Начать с каждого объекта в отдельном кластере.\n",
        "2. Найти ближайшие кластеры и объединить их.\n",
        "3. Повторять шаг 2 до тех пор, пока не останется один кластер или не будет достигнуто заданное количество кластеров.\n",
        "\n",
        "**Преимущества**:\n",
        "- Не требует заранее заданного числа кластеров.\n",
        "- Дерево кластеров (дендрограмма) может предоставить визуальную информацию о структуре данных.\n",
        "\n",
        "**Недостатки**:\n",
        "- Высокая вычислительная сложность.\n",
        "- Чувствителен к выбросам и шуму.\n",
        "\n",
        "**Пример использования агломеративной кластеризации на Python**:\n",
        "\n",
        "```python\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import scipy.cluster.hierarchy as sch\n",
        "\n",
        "# Применение агломеративной кластеризации\n",
        "agg_clust = AgglomerativeClustering(n_clusters=3)\n",
        "labels = agg_clust.fit_predict(X)\n",
        "\n",
        "# Визуализация дендрограммы\n",
        "plt.figure(figsize=(10, 7))\n",
        "dendrogram = sch.dendrogram(sch.linkage(X, method='ward'))\n",
        "plt.show()\n",
        "\n",
        "# Визуализация результатов кластеризации\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "### Заключение\n",
        "\n",
        "Кластеризация – это важный метод анализа данных, позволяющий выявить скрытые структуры и группы объектов. Разные алгоритмы кластеризации имеют свои преимущества и недостатки, и выбор подходящего алгоритма зависит от конкретной задачи, структуры данных и требований к результатам."
      ],
      "metadata": {
        "id": "ko6fmlCUmK6j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Метод ближайших соседей (k-Nearest Neighbors, k-NN)\n",
        "\n",
        "Метод ближайших соседей (k-NN) — это простой и понятный алгоритм машинного обучения, используемый как для задач классификации, так и для регрессии. Принцип его работы заключается в следующем:\n",
        "\n",
        "1. Для заданной точки \\( x \\), которую нужно классифицировать или для которой нужно предсказать значение, находим \\( k \\) ближайших точек (соседей) из обучающей выборки.\n",
        "2. В случае классификации:\n",
        "   - Классифицируем точку \\( x \\) по большинству классов среди \\( k \\) ближайших соседей.\n",
        "3. В случае регрессии:\n",
        "   - Предсказываем значение как среднее значение среди \\( k \\) ближайших соседей.\n",
        "\n",
        "**Пример использования k-NN на Python**:\n",
        "\n",
        "```python\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Загрузка данных\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Разделение данных на тренировочную и тестовую выборки\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Обучение модели k-NN\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Предсказание и оценка модели\n",
        "y_pred = knn.predict(X_test)\n",
        "```\n",
        "\n",
        "### Leave-One-Out Кросс-Валидация\n",
        "\n",
        "Leave-One-Out (LOO) кросс-валидация — это особый случай k-Fold кросс-валидации, где \\( k \\) равно количеству всех образцов в наборе данных. То есть, для каждого образца из набора данных, модель обучается на всех остальных образцах и тестируется на этом одном образце.\n",
        "\n",
        "**Преимущества LOO**:\n",
        "- Модель обучается и тестируется на каждом возможном подмножестве данных, что делает оценку очень точной.\n",
        "\n",
        "**Недостатки LOO**:\n",
        "- Очень вычислительно затратен для больших наборов данных.\n",
        "\n",
        "**Пример использования LOO на Python**:\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import LeaveOneOut\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Загрузка данных\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Leave-One-Out кросс-валидация\n",
        "loo = LeaveOneOut()\n",
        "y_true, y_pred = [], []\n",
        "\n",
        "for train_index, test_index in loo.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    \n",
        "    knn = KNeighborsClassifier(n_neighbors=3)\n",
        "    knn.fit(X_train, y_train)\n",
        "    y_pred.append(knn.predict(X_test)[0])\n",
        "    y_true.append(y_test[0])\n",
        "\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "```\n",
        "\n",
        "### k-Fold Кросс-Валидация\n",
        "\n",
        "k-Fold кросс-валидация делит данные на \\( k \\) равных подмножеств (фолдов). В каждом из \\( k \\) циклов один из фолдов используется как тестовая выборка, а остальные \\( k-1 \\) фолдов — как обучающая выборка.\n",
        "\n",
        "**Преимущества k-Fold**:\n",
        "- Более стабильная оценка производительности модели по сравнению с обычным разделением на тренировочную и тестовую выборки.\n",
        "- Все данные используются как для обучения, так и для тестирования.\n",
        "\n",
        "**Недостатки k-Fold**:\n",
        "- Более вычислительно затратен по сравнению с обычным разделением данных.\n",
        "\n",
        "**Пример использования k-Fold на Python**:\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Загрузка данных\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# k-Fold кросс-валидация\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "scores = cross_val_score(knn, X, y, cv=kf, scoring='accuracy')\n",
        "\n",
        "print(f\"Cross-Validation Accuracy Scores: {scores}\")\n",
        "print(f\"Mean Accuracy: {scores.mean():.2f}\")\n",
        "```\n",
        "\n",
        "### Заключение\n",
        "\n",
        "Метод ближайших соседей (k-NN) — это простой и эффективный алгоритм, который можно использовать как для классификации, так и для регрессии. Однако он может быть вычислительно затратным для больших наборов данных и чувствителен к выбору гиперпараметра \\( k \\).\n",
        "\n",
        "Кросс-валидация, включая Leave-One-Out и k-Fold, позволяет получить более точную оценку производительности модели, уменьшая риск переобучения и недообучения. Эти методы широко применяются для оценки и выбора моделей в задачах машинного обучения."
      ],
      "metadata": {
        "id": "knDY164XmW-p"
      }
    }
  ]
}